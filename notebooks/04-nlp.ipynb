{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ac9645-0e27-4d7f-92c7-0c3a4e0cc680",
   "metadata": {},
   "source": [
    "# Exploring NLP tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb7ca8a-ac69-4508-95d6-eb97ac192b01",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ccb1b-78a5-48fc-b5af-b984ec42da8c",
   "metadata": {},
   "source": [
    "### Install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c591de-a9c0-4311-b9cb-0502f79021c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:17:45.224801Z",
     "iopub.status.busy": "2022-09-14T04:17:45.224348Z",
     "iopub.status.idle": "2022-09-14T04:17:47.998753Z",
     "shell.execute_reply": "2022-09-14T04:17:47.997629Z",
     "shell.execute_reply.started": "2022-09-14T04:17:45.224731Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from nltk) (2022.9.13)\n",
      "Requirement already satisfied: click in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from nltk) (8.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294d2ad9-c034-43f1-8560-ee40a8778a41",
   "metadata": {},
   "source": [
    "### Explore NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "101912e4-a73e-48a9-8e65-c2eceb1a82d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:17:48.000994Z",
     "iopub.status.busy": "2022-09-14T04:17:48.000635Z",
     "iopub.status.idle": "2022-09-14T04:17:51.771359Z",
     "shell.execute_reply": "2022-09-14T04:17:51.770391Z",
     "shell.execute_reply.started": "2022-09-14T04:17:48.000960Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                 ALIEN\n",
      "\n",
      "                          BY GEORGE O. SMITH\n",
      "\n",
      "           [Transcriber's Note: This etext was produced from\n",
      "               Astounding Science-Fiction, October 1946.\n",
      "         Extensive research did not uncover any evidence that\n",
      "         the U.S. copyright on this publication was renewed.]\n",
      "\n",
      "\n",
      "The telephone rang and the lieutenant of police Timothy McDowell\n",
      "grunted. He put down his magazine, and hastily covered the\n",
      "partially-clad damsel on the front cover before he answered the ringing\n",
      "phone.\n",
      "\n",
      "\"McDowell,\" he grunted.\n",
      "\n",
      "\"McDowell,\" came the voice in his ear. \"I think ye'd better come overe\n",
      "here.\"\n",
      "\n",
      "\"What's up?\"\n",
      "\n",
      "\"Been a riot at McCarthy's on Boylston Street.\"\n",
      "\n",
      "\"That's nothing new,\" growled McDowell, \"excepting sometimes it's\n",
      "Hennesey's on Dartmouth or Kelley's on Massachusetts.\"\n",
      "\n",
      "\"Yeah, but this is different.\"\n",
      "\n",
      "\"Whut's so different about a riot in a jernt like McCarthy's on a\n",
      "street like Boylston?\"\n",
      "\n",
      "\"Well, the \n",
      "--------------------------------------------------------------------------------\n",
      "es in evolutionary ending of\n",
      "the host. Then, with the scalp renewed by the so-called Rotation of\n",
      "Crops.\"\n",
      "\n",
      "\"Uh-huh. Well, we'll let the jury decide!\"\n",
      "\n",
      "Two months elapsed before O'Toole came to trial. But meantime, the\n",
      "judge took a vacation and returned with a luxuriant growth of hair on\n",
      "his head. The jury was not cited for contempt of court even though most\n",
      "of them insisted on keeping their hats on during proceedings. O'Toole\n",
      "had a good lawyer.\n",
      "\n",
      "And Judge Murphy beamed down over the bench and said: \"O'Toole, you\n",
      "are guilty, but sentence is suspended indefinitely. Just don't get\n",
      "into trouble again, that's all. And gentlemen, Lieutenant McDowell,\n",
      "Dr. Muldoon, and Sergeant O'Leary, I commend all of your work and will\n",
      "direct that you, Mr. McCarthy, be recompensed. As for you,\" he said\n",
      "to the ex-featherhead. \"Mr. William B. Windsor, we have no use for\n",
      "foreigners--\"\n",
      "\n",
      "Mr. Windsor never got a chance to state that he was no foreigner; his\n",
      "mother was a Clancy.\n",
      "\n",
      "THE END.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a raw plaintext corpus from Project Gutenberg\n",
    "import requests\n",
    "import nltk\n",
    "\n",
    "f = requests.get('https://www.gutenberg.org/cache/epub/68196/pg68196.txt')\n",
    "start_str = \"*** START OF THE PROJECT GUTENBERG EBOOK ALIEN ***\"\n",
    "start = f.text.find(start_str) + len(start_str)\n",
    "end = f.text.rfind(\"*** END OF THE PROJECT GUTENBERG EBOOK ALIEN ***\")\n",
    "raw_text = f.text[start:end]\n",
    "print(raw_text[:1000])\n",
    "print('-' * 80)\n",
    "print(raw_text[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a03d99ed-a832-4310-8e9e-5efe901b184c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:17:51.778480Z",
     "iopub.status.busy": "2022-09-14T04:17:51.778038Z",
     "iopub.status.idle": "2022-09-14T04:17:51.882083Z",
     "shell.execute_reply": "2022-09-14T04:17:51.881115Z",
     "shell.execute_reply.started": "2022-09-14T04:17:51.778452Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ALIEN',\n",
       "  'BY',\n",
       "  'GEORGE',\n",
       "  'O.',\n",
       "  'SMITH',\n",
       "  '[',\n",
       "  'Transcriber',\n",
       "  \"'s\",\n",
       "  'Note',\n",
       "  ':',\n",
       "  'This',\n",
       "  'etext',\n",
       "  'was',\n",
       "  'produced',\n",
       "  'from',\n",
       "  'Astounding',\n",
       "  'Science-Fiction',\n",
       "  ',',\n",
       "  'October',\n",
       "  '1946',\n",
       "  '.'],\n",
       " ['Extensive',\n",
       "  'research',\n",
       "  'did',\n",
       "  'not',\n",
       "  'uncover',\n",
       "  'any',\n",
       "  'evidence',\n",
       "  'that',\n",
       "  'the',\n",
       "  'U.S.',\n",
       "  'copyright',\n",
       "  'on',\n",
       "  'this',\n",
       "  'publication',\n",
       "  'was',\n",
       "  'renewed',\n",
       "  '.',\n",
       "  ']']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize our corpus into sentences and tokens (list of lists)\n",
    "sent_toks = []\n",
    "for s in nltk.sent_tokenize(raw_text):\n",
    "    sent_toks.append(nltk.word_tokenize(s))\n",
    "sent_toks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75ecd65-dd2e-4671-b5a8-fe504f9f4e1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:17:51.884031Z",
     "iopub.status.busy": "2022-09-14T04:17:51.883447Z",
     "iopub.status.idle": "2022-09-14T04:17:52.219059Z",
     "shell.execute_reply": "2022-09-14T04:17:52.218167Z",
     "shell.execute_reply.started": "2022-09-14T04:17:51.884005Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 1228 items>\n",
      "<NgramCounter with 4 ngram orders and 26666 ngrams>\n",
      "CPU times: user 325 ms, sys: 7.79 ms, total: 333 ms\n",
      "Wall time: 330 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a maximum likelihood expectation (MLE) language model from the corpus\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "N = 4\n",
    "train, vocab = padded_everygram_pipeline(N, sent_toks)\n",
    "\n",
    "lm = MLE(N)\n",
    "lm.fit(train, vocab)\n",
    "print(lm.vocab)\n",
    "print(lm.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2b1dbe-451d-419f-bee8-35a891555a31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:17:52.220799Z",
     "iopub.status.busy": "2022-09-14T04:17:52.220436Z",
     "iopub.status.idle": "2022-09-14T04:17:52.228253Z",
     "shell.execute_reply": "2022-09-14T04:17:52.227407Z",
     "shell.execute_reply.started": "2022-09-14T04:17:52.220772Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"that you were afraid , that you'd been hiding because of the differences in evolutionary ending of the host . </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate some text from the language model using a seed 2-gram\n",
    "toks = lm.generate(32, text_seed='Tell them'.split(), random_seed=42)\n",
    "' '.join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b4cb0a0-47d6-4ffd-84ba-6499656b19e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:17:52.230142Z",
     "iopub.status.busy": "2022-09-14T04:17:52.229832Z",
     "iopub.status.idle": "2022-09-14T04:17:52.235978Z",
     "shell.execute_reply": "2022-09-14T04:17:52.235208Z",
     "shell.execute_reply.started": "2022-09-14T04:17:52.230115Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.8073549220576046"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for log likelihood this sequence exists in the language model\n",
    "lm.logscore('bit', 'was a'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913b2861-8a0c-40b4-80e3-82ba0f338910",
   "metadata": {},
   "source": [
    "## GenSim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f31f517-1716-4dd3-bd99-5dac7818762c",
   "metadata": {},
   "source": [
    "### Install GenSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bd03c42-bc2b-45fb-baf2-2cf4aa1492bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:17:52.237812Z",
     "iopub.status.busy": "2022-09-14T04:17:52.237466Z",
     "iopub.status.idle": "2022-09-14T04:17:54.974497Z",
     "shell.execute_reply": "2022-09-14T04:17:54.973314Z",
     "shell.execute_reply.started": "2022-09-14T04:17:52.237786Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from gensim) (1.23.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e88a102-d77c-42cd-b42b-9816ff8f536f",
   "metadata": {},
   "source": [
    "### Explore GenSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e66422dd-6885-4005-995d-571b74d1cc1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:17:54.976848Z",
     "iopub.status.busy": "2022-09-14T04:17:54.976380Z",
     "iopub.status.idle": "2022-09-14T04:17:55.890934Z",
     "shell.execute_reply": "2022-09-14T04:17:55.890064Z",
     "shell.execute_reply.started": "2022-09-14T04:17:54.976812Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences=sent_toks, vector_size=5)\n",
    "word_vectors = model.wv\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1fa699b-472c-4fda-9b5f-b6f3bb74d508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:17:55.892789Z",
     "iopub.status.busy": "2022-09-14T04:17:55.892400Z",
     "iopub.status.idle": "2022-09-14T04:17:55.900601Z",
     "shell.execute_reply": "2022-09-14T04:17:55.899809Z",
     "shell.execute_reply.started": "2022-09-14T04:17:55.892761Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/156 is ., vec = [ 0.19978775  0.85359436  1.1713744  -0.9864809  -0.3433871 ]\n",
      "word #1/156 is ,, vec = [ 0.1172794  1.1657429  1.4586245 -1.5009246 -0.2580464]\n",
      "word #2/156 is the, vec = [ 0.43704826  1.0936207   1.3120347  -1.3994651  -0.29396105]\n",
      "word #3/156 is '', vec = [ 0.12169423  0.6942297   0.7988962  -1.0238564  -0.2958518 ]\n",
      "word #4/156 is ``, vec = [ 3.2935810e-01  8.1392103e-01  1.0268925e+00 -9.5124507e-01\n",
      "  4.7777989e-04]\n",
      "word #5/156 is a, vec = [ 0.09903629  0.63957345  0.93133605 -1.0325854  -0.19885504]\n",
      "word #6/156 is and, vec = [ 0.02612713  0.6771986   1.0611243  -1.1080297  -0.17047247]\n",
      "word #7/156 is to, vec = [ 0.13124461  0.84318745  0.7264108  -0.9244929  -0.21209371]\n",
      "word #8/156 is of, vec = [ 0.05099332  1.037147    0.98514247 -1.3549186  -0.16270308]\n",
      "word #9/156 is that, vec = [ 0.18680783  0.59298074  1.1109077  -0.9128647   0.05257313]\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(word_vectors.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(word_vectors.index_to_key)} is {word}, vec = {word_vectors[word]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "439ce33c-f644-4073-88b9-470d498fdedc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:17:55.902298Z",
     "iopub.status.busy": "2022-09-14T04:17:55.901939Z",
     "iopub.status.idle": "2022-09-14T04:17:55.909193Z",
     "shell.execute_reply": "2022-09-14T04:17:55.907946Z",
     "shell.execute_reply.started": "2022-09-14T04:17:55.902273Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The = [ 0.26197264  0.20541258  0.47735476 -0.457172   -0.0888974 ]\n",
      "jury\n",
      "was = [-0.02212266  0.4195795   0.618374   -0.5979918  -0.19352509]\n",
      "not = [ 0.2595777   0.38216555  0.62206984 -0.79241335  0.03801519]\n",
      "cited\n",
      "for = [ 0.29584187  0.692679    0.77136207 -0.712972    0.05781078]\n",
      "contempt\n",
      "of = [ 0.05099332  1.037147    0.98514247 -1.3549186  -0.16270308]\n",
      "court\n"
     ]
    }
   ],
   "source": [
    "tokens = \"The jury was not cited for contempt of court\".split()\n",
    "for token in tokens:\n",
    "    if token in word_vectors:\n",
    "        print(f'{token} = {word_vectors[token]}')\n",
    "    else:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8690fdd-3aef-4261-830b-4c134881c484",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-09-14T04:17:55.911144Z",
     "iopub.status.busy": "2022-09-14T04:17:55.910813Z",
     "iopub.status.idle": "2022-09-14T04:17:56.090331Z",
     "shell.execute_reply": "2022-09-14T04:17:56.089380Z",
     "shell.execute_reply.started": "2022-09-14T04:17:55.911116Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fasttext-wiki-news-subwords-300': {'num_records': 999999,\n",
       "  'file_size': 1005007116,\n",
       "  'base_dataset': 'Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py',\n",
       "  'license': 'https://creativecommons.org/licenses/by-sa/3.0/',\n",
       "  'parameters': {'dimension': 300},\n",
       "  'description': '1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).',\n",
       "  'read_more': ['https://fasttext.cc/docs/en/english-vectors.html',\n",
       "   'https://arxiv.org/abs/1712.09405',\n",
       "   'https://arxiv.org/abs/1607.01759'],\n",
       "  'checksum': 'de2bb3a20c46ce65c9c131e1ad9a77af',\n",
       "  'file_name': 'fasttext-wiki-news-subwords-300.gz',\n",
       "  'parts': 1},\n",
       " 'conceptnet-numberbatch-17-06-300': {'num_records': 1917247,\n",
       "  'file_size': 1225497562,\n",
       "  'base_dataset': 'ConceptNet, word2vec, GloVe, and OpenSubtitles 2016',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py',\n",
       "  'license': 'https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt',\n",
       "  'parameters': {'dimension': 300},\n",
       "  'description': 'ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.',\n",
       "  'read_more': ['http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972',\n",
       "   'https://github.com/commonsense/conceptnet-numberbatch',\n",
       "   'http://conceptnet.io/'],\n",
       "  'checksum': 'fd642d457adcd0ea94da0cd21b150847',\n",
       "  'file_name': 'conceptnet-numberbatch-17-06-300.gz',\n",
       "  'parts': 1},\n",
       " 'word2vec-ruscorpora-300': {'num_records': 184973,\n",
       "  'file_size': 208427381,\n",
       "  'base_dataset': 'Russian National Corpus (about 250M words)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py',\n",
       "  'license': 'https://creativecommons.org/licenses/by/4.0/deed.en',\n",
       "  'parameters': {'dimension': 300, 'window_size': 10},\n",
       "  'description': 'Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.',\n",
       "  'preprocessing': 'The corpus was lemmatized and tagged with Universal PoS',\n",
       "  'read_more': ['https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models',\n",
       "   'http://rusvectores.org/en/',\n",
       "   'https://github.com/RaRe-Technologies/gensim-data/issues/3'],\n",
       "  'checksum': '9bdebdc8ae6d17d20839dd9b5af10bc4',\n",
       "  'file_name': 'word2vec-ruscorpora-300.gz',\n",
       "  'parts': 1},\n",
       " 'word2vec-google-news-300': {'num_records': 3000000,\n",
       "  'file_size': 1743563840,\n",
       "  'base_dataset': 'Google News (about 100 billion words)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py',\n",
       "  'license': 'not found',\n",
       "  'parameters': {'dimension': 300},\n",
       "  'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
       "  'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
       "   'https://arxiv.org/abs/1301.3781',\n",
       "   'https://arxiv.org/abs/1310.4546',\n",
       "   'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
       "  'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
       "  'file_name': 'word2vec-google-news-300.gz',\n",
       "  'parts': 1},\n",
       " 'glove-wiki-gigaword-50': {'num_records': 400000,\n",
       "  'file_size': 69182535,\n",
       "  'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 50},\n",
       "  'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
       "  'file_name': 'glove-wiki-gigaword-50.gz',\n",
       "  'parts': 1},\n",
       " 'glove-wiki-gigaword-100': {'num_records': 400000,\n",
       "  'file_size': 134300434,\n",
       "  'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 100},\n",
       "  'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': '40ec481866001177b8cd4cb0df92924f',\n",
       "  'file_name': 'glove-wiki-gigaword-100.gz',\n",
       "  'parts': 1},\n",
       " 'glove-wiki-gigaword-200': {'num_records': 400000,\n",
       "  'file_size': 264336934,\n",
       "  'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 200},\n",
       "  'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': '59652db361b7a87ee73834a6c391dfc1',\n",
       "  'file_name': 'glove-wiki-gigaword-200.gz',\n",
       "  'parts': 1},\n",
       " 'glove-wiki-gigaword-300': {'num_records': 400000,\n",
       "  'file_size': 394362229,\n",
       "  'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 300},\n",
       "  'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': '29e9329ac2241937d55b852e8284e89b',\n",
       "  'file_name': 'glove-wiki-gigaword-300.gz',\n",
       "  'parts': 1},\n",
       " 'glove-twitter-25': {'num_records': 1193514,\n",
       "  'file_size': 109885004,\n",
       "  'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 25},\n",
       "  'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': '50db0211d7e7a2dcd362c6b774762793',\n",
       "  'file_name': 'glove-twitter-25.gz',\n",
       "  'parts': 1},\n",
       " 'glove-twitter-50': {'num_records': 1193514,\n",
       "  'file_size': 209216938,\n",
       "  'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 50},\n",
       "  'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': 'c168f18641f8c8a00fe30984c4799b2b',\n",
       "  'file_name': 'glove-twitter-50.gz',\n",
       "  'parts': 1},\n",
       " 'glove-twitter-100': {'num_records': 1193514,\n",
       "  'file_size': 405932991,\n",
       "  'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 100},\n",
       "  'description': 'Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': 'b04f7bed38756d64cf55b58ce7e97b15',\n",
       "  'file_name': 'glove-twitter-100.gz',\n",
       "  'parts': 1},\n",
       " 'glove-twitter-200': {'num_records': 1193514,\n",
       "  'file_size': 795373100,\n",
       "  'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 200},\n",
       "  'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': 'e52e8392d1860b95d5308a525817d8f9',\n",
       "  'file_name': 'glove-twitter-200.gz',\n",
       "  'parts': 1},\n",
       " '__testing_word2vec-matrix-synopsis': {'description': '[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.',\n",
       "  'parameters': {'dimensions': 50},\n",
       "  'preprocessing': 'Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`.',\n",
       "  'read_more': [],\n",
       "  'checksum': '534dcb8b56a360977a269b7bfc62d124',\n",
       "  'file_name': '__testing_word2vec-matrix-synopsis.gz',\n",
       "  'parts': 1}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "api.info()['models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65a62382-b606-4b14-9654-e6e5398f9ce9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:17:56.092459Z",
     "iopub.status.busy": "2022-09-14T04:17:56.091879Z",
     "iopub.status.idle": "2022-09-14T04:18:20.714161Z",
     "shell.execute_reply": "2022-09-14T04:18:20.713313Z",
     "shell.execute_reply.started": "2022-09-14T04:17:56.092432Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.2 s, sys: 341 ms, total: 24.5 s\n",
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-50\")  # load pre-trained word-vectors from gensim-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ef8139b-0dbe-4197-9d5e-21b6ac600b69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:18:20.716034Z",
     "iopub.status.busy": "2022-09-14T04:18:20.715624Z",
     "iopub.status.idle": "2022-09-14T04:18:20.736470Z",
     "shell.execute_reply": "2022-09-14T04:18:20.735354Z",
     "shell.execute_reply.started": "2022-09-14T04:18:20.716006Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "jury = [ 0.13889   -0.15441   -0.65196    0.55466    1.2798     0.60057\n",
      "  0.7441     1.4171     0.32741    0.40929   -0.69933   -0.42597\n",
      " -0.40659    0.22372    1.358     -0.5151    -0.24794   -0.48514\n",
      " -0.44527   -1.2945     1.1523     0.86963    0.63349   -0.072768\n",
      " -1.0655    -1.9788    -0.35197    0.21787   -0.83935   -0.95735\n",
      "  1.1546    -0.96692   -0.67812   -1.8802     0.89337   -0.91028\n",
      "  0.70292    0.0085246  0.49123   -0.95192   -0.56366    0.19392\n",
      "  0.29582    0.74449   -0.76221   -0.16316   -0.28296    0.082915\n",
      "  0.047064   0.20868  ]\n",
      "was = [ 0.086888 -0.19416  -0.24267  -0.33391   0.56731   0.39783  -0.97809\n",
      "  0.03159  -0.61469  -0.31406   0.56145   0.12886  -0.84193  -0.46992\n",
      "  0.47097   0.023012 -0.59609   0.22291  -1.1614    0.3865    0.067412\n",
      "  0.44883   0.17394  -0.53574   0.17909  -2.1647   -0.12827   0.29036\n",
      " -0.15061   0.35242   3.124    -0.90085  -0.02567  -0.41709   0.40565\n",
      " -0.22703   0.76829   0.60982   0.070068 -0.13271  -0.1201    0.096132\n",
      " -0.43998  -0.48531  -0.5188   -0.3077   -0.75028  -0.77      0.3945\n",
      " -0.16937 ]\n",
      "not = [ 5.5025e-01 -2.4942e-01 -9.3860e-04 -2.6400e-01  5.9320e-01  2.7950e-01\n",
      " -2.5666e-01  9.3076e-02 -3.6288e-01  9.0776e-02  2.8409e-01  7.1337e-01\n",
      " -4.7510e-01 -2.4413e-01  8.8424e-01  8.9109e-01  4.3009e-01 -2.7330e-01\n",
      "  1.1276e-01 -8.1665e-01 -4.1272e-01  1.7754e-01  6.1942e-01  1.0466e-01\n",
      "  3.3327e-01 -2.3125e+00 -5.2371e-01 -2.1898e-02  5.3801e-01 -5.0615e-01\n",
      "  3.8683e+00  1.6642e-01 -7.1981e-01 -7.4728e-01  1.1631e-01 -3.7585e-01\n",
      "  5.5520e-01  1.2675e-01 -2.2642e-01 -1.0175e-01 -3.5455e-01  1.2348e-01\n",
      "  1.6532e-01  7.0420e-01 -8.0231e-02 -6.8406e-02 -6.7626e-01  3.3763e-01\n",
      "  5.0139e-02  3.3465e-01]\n",
      "cited = [-2.0528e-01  2.7257e-01  1.4962e-03  2.9793e-01 -2.7053e-01  3.5055e-01\n",
      " -4.2643e-01 -1.9070e-01  2.7908e-01 -2.3309e-01 -2.9525e-01 -8.4476e-02\n",
      " -4.4230e-01 -5.1410e-01  6.0720e-01 -5.0147e-01 -6.4872e-01 -7.3953e-01\n",
      "  3.4284e-01 -2.1567e-01  1.1927e-01  2.8435e-02  2.8564e-01 -5.2266e-01\n",
      "  1.0404e-01 -1.5128e+00 -6.9637e-01 -4.5320e-02 -7.3688e-01  6.9118e-01\n",
      "  2.2044e+00 -7.6660e-01  6.4987e-01 -1.3005e+00 -2.4407e-01 -4.3881e-01\n",
      "  1.5818e-01 -3.3593e-01 -2.6296e-01  4.0455e-01 -8.2732e-02  5.0535e-01\n",
      "  2.6084e-01  8.1399e-02  4.2600e-01 -2.7255e-01 -7.7901e-01  1.5297e+00\n",
      "  2.5773e-01  3.1793e-01]\n",
      "for = [ 0.15272   0.36181  -0.22168   0.066051  0.13029   0.37075  -0.75874\n",
      " -0.44722   0.22563   0.10208   0.054225  0.13494  -0.43052  -0.2134\n",
      "  0.56139  -0.21445   0.077974  0.10137  -0.51306  -0.40295   0.40639\n",
      "  0.23309   0.20696  -0.12668  -0.50634  -1.7131    0.077183 -0.39138\n",
      " -0.10594  -0.23743   3.9552    0.66596  -0.61841  -0.3268    0.37021\n",
      "  0.25764   0.38977   0.27121   0.043024 -0.34322   0.020339  0.2142\n",
      "  0.044097  0.14003  -0.20079   0.074794 -0.36076   0.43382  -0.084617\n",
      "  0.1214  ]\n",
      "contempt = [-0.82267   0.39537  -0.70574  -0.38362   0.47295   0.33293   1.6421\n",
      "  0.81708   0.10963   0.46733  -0.44187   0.11329   0.015812 -0.83256\n",
      "  0.56833  -0.32634  -0.31429  -0.36985   0.13411  -1.0155    0.43571\n",
      "  0.96228   0.80143  -0.60078  -0.41166  -1.8792   -0.20294   0.42079\n",
      "  0.11012   0.3211    0.44841  -0.13318  -0.58893  -1.3363   -0.50937\n",
      " -0.79231  -0.020265 -1.0515   -0.16728  -0.61297   0.42466   0.2019\n",
      "  0.40808   1.1104   -0.21935  -0.40475  -0.61488   0.29991  -0.043222\n",
      " -0.68664 ]\n",
      "of = [ 0.70853    0.57088   -0.4716     0.18048    0.54449    0.72603\n",
      "  0.18157   -0.52393    0.10381   -0.17566    0.078852  -0.36216\n",
      " -0.11829   -0.83336    0.11917   -0.16605    0.061555  -0.012719\n",
      " -0.56623    0.013616   0.22851   -0.14396   -0.067549  -0.38157\n",
      " -0.23698   -1.7037    -0.86692   -0.26704   -0.2589     0.1767\n",
      "  3.8676    -0.1613    -0.13273   -0.68881    0.18444    0.0052464\n",
      " -0.33874   -0.078956   0.24185    0.36576   -0.34727    0.28483\n",
      "  0.075693  -0.062178  -0.38988    0.22902   -0.21617   -0.22562\n",
      " -0.093918  -0.80375  ]\n",
      "court = [-0.35445   -0.55181   -1.024      0.59384    0.57911   -0.061788\n",
      "  0.72901    1.2659    -0.13373   -0.34189   -0.45832   -0.32753\n",
      " -0.50637   -0.40303    0.96671    0.23091   -0.60122   -0.9605\n",
      "  0.0088542 -0.43992    0.86799    0.53513   -0.3755     0.098895\n",
      " -0.95862   -3.0575     0.13488   -0.27774   -0.87982   -0.4781\n",
      "  2.588     -1.0412    -0.76467   -0.83672    0.37049   -0.58154\n",
      "  0.94555   -0.0082917  0.051426  -0.37127   -0.15439    0.67224\n",
      "  0.57069    0.80539   -0.7087     0.28794   -0.62886   -0.47116\n",
      "  0.13036   -0.33523  ]\n"
     ]
    }
   ],
   "source": [
    "tokens = \"The jury was not cited for contempt of court\".split()\n",
    "for token in tokens:\n",
    "    if token in word_vectors:\n",
    "        print(f'{token} = {word_vectors[token]}')\n",
    "    else:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e96ba77-4058-419a-8790-1591b51d93a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:18:20.738438Z",
     "iopub.status.busy": "2022-09-14T04:18:20.738024Z",
     "iopub.status.idle": "2022-09-14T04:18:20.745874Z",
     "shell.execute_reply": "2022-09-14T04:18:20.744768Z",
     "shell.execute_reply.started": "2022-09-14T04:18:20.738409Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen / calc_queen [0.86095816]\n",
      "man / woman 0.11396622657775879\n",
      "king / queen 0.2160956859588623\n"
     ]
    }
   ],
   "source": [
    "king = word_vectors['king']\n",
    "man = word_vectors['man']\n",
    "woman = word_vectors['woman']\n",
    "queen = word_vectors['queen']\n",
    "\n",
    "calc_queen = king - man + woman\n",
    "print('queen / calc_queen', word_vectors.cosine_similarities(queen, [calc_queen]))\n",
    "print('man / woman', word_vectors.distance('man', 'woman'))\n",
    "print('king / queen', word_vectors.distance('king', 'queen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0eabdf-6d3b-40ca-8d8b-e69a844a4772",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65b601d-f866-4be0-9f75-543e63e8382b",
   "metadata": {},
   "source": [
    "### Install spaCy tools and starter English language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05f1b198-da01-4250-820c-985b9a6ee8d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:18:20.748023Z",
     "iopub.status.busy": "2022-09-14T04:18:20.747632Z",
     "iopub.status.idle": "2022-09-14T04:18:35.266124Z",
     "shell.execute_reply": "2022-09-14T04:18:35.264790Z",
     "shell.execute_reply.started": "2022-09-14T04:18:20.747996Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (22.2.2)\n",
      "Requirement already satisfied: setuptools in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (65.3.0)\n",
      "Requirement already satisfied: wheel in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (0.37.1)\n",
      "Requirement already satisfied: spacy in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (3.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (1.23.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (1.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: setuptools in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (65.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (1.9.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (3.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (8.1.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.1)\n",
      "Requirement already satisfied: blis<0.10.0,>=0.7.8 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.9.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.23.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
      "Requirement already satisfied: setuptools in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (65.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: blis<0.10.0,>=0.7.8 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.9.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.0.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rbates/src/adc2022/ENV/lib/python3.10/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef48fd-4459-4f66-b09f-89e454e98e62",
   "metadata": {},
   "source": [
    "### Explore spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "add1c37f-d131-41ff-83c2-07d817fc3247",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:18:35.268862Z",
     "iopub.status.busy": "2022-09-14T04:18:35.268446Z",
     "iopub.status.idle": "2022-09-14T04:18:37.175803Z",
     "shell.execute_reply": "2022-09-14T04:18:37.174952Z",
     "shell.execute_reply.started": "2022-09-14T04:18:35.268827Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP dobj X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09dd415e-2cc8-42a1-9e11-ee0d7ac53694",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:18:37.177786Z",
     "iopub.status.busy": "2022-09-14T04:18:37.177315Z",
     "iopub.status.idle": "2022-09-14T04:18:37.193739Z",
     "shell.execute_reply": "2022-09-14T04:18:37.192886Z",
     "shell.execute_reply.started": "2022-09-14T04:18:37.177758Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars cars nsubj shift\n",
      "insurance liability liability dobj shift\n",
      "manufacturers manufacturers pobj toward\n"
     ]
    }
   ],
   "source": [
    "# Chunking text on subphrases\n",
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3ebc832-c3f6-41b0-9466-94c7842dfae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:18:37.195944Z",
     "iopub.status.busy": "2022-09-14T04:18:37.195505Z",
     "iopub.status.idle": "2022-09-14T04:18:37.211374Z",
     "shell.execute_reply": "2022-09-14T04:18:37.210432Z",
     "shell.execute_reply.started": "2022-09-14T04:18:37.195915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "# Named entity recognition\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "633b9579-e32b-4ffa-b32a-ff8f531a9b69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T04:18:37.213452Z",
     "iopub.status.busy": "2022-09-14T04:18:37.213077Z",
     "iopub.status.idle": "2022-09-14T04:18:37.229183Z",
     "shell.execute_reply": "2022-09-14T04:18:37.228360Z",
     "shell.execute_reply.started": "2022-09-14T04:18:37.213426Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The True 10.318105 True\n",
      "fat True 8.668562 True\n",
      "cat True 8.584026 True\n",
      "sat True 7.815958 True\n",
      "on True 8.546586 True\n",
      "the True 8.815763 True\n",
      "mat True 8.037259 True\n"
     ]
    }
   ],
   "source": [
    "# spaCy also has word vectors in its language models\n",
    "tokens = nlp(\"The fat cat sat on the mat\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f40feb-eb09-43b2-a4da-d412d7ecc5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
